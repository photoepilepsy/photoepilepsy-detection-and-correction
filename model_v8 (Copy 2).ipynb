{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a48d22-54f2-46e4-a21f-182e3ed2f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlabelling upd\\npos encoder upd\\nlr dec\\ndropout inv\\n\\n\\nif random.random() > 0.5:\\n    frames = frames[::-1]  # Reverse sequence\\n\\nFEATURE_DIM = 256 \\nlr inc\\n\\nfut:\\ndecrase lr \\ninc epoch\\n\\nhand tailed labelas\\n\\nsmooth labels can be used\\n\\nlstm exper\\nmin exp\\n\\n2x more data.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "labelling upd\n",
    "pos encoder upd\n",
    "lr dec\n",
    "dropout inv\n",
    "\n",
    "\n",
    "if random.random() > 0.5:\n",
    "    frames = frames[::-1]  # Reverse sequence\n",
    "\n",
    "FEATURE_DIM = 256 \n",
    "lr inc\n",
    "\n",
    "fut:\n",
    "decrase lr \n",
    "inc epoch\n",
    "\n",
    "hand tailed labelas\n",
    "\n",
    "smooth labels can be used\n",
    "\n",
    "lstm exper\n",
    "min exp\n",
    "\n",
    "2x more data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f857c6-4294-45e5-94da-80b6cfa3785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a30700-65a2-4b5a-89bf-057423db75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "VIDEOS_DIR = '/home/kaan-eren/projects/V4_BITIRME'\n",
    "MAX_FRAMES = 30  # Frame window size\n",
    "FRAME_SIZE = (224, 224)  # Standard size for most pre-trained models\n",
    "BATCH_SIZE = 4  # Adjust based on VRAM\n",
    "NUM_WORKERS = 1  \n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "FEATURE_DIM = 256  # Dimensionality of frame features\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "NUM_LAYERS = 3  # Number of transformer layers\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9d629-9c4c-4523-a01f-7a32e5d3a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for video frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(FRAME_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76752a46-554a-4b88-8e70-8fec0ebff07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle video data for effect detection model\n",
    "    \n",
    "    This dataset processes an entire video file and extracts frames at regular intervals,\n",
    "    creating a dataset of consecutive frame windows for training the effect detection model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        json_data: List[Dict], \n",
    "        videos_dir: str,\n",
    "        max_frames: int = MAX_FRAMES,\n",
    "        frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "        transform=None,\n",
    "        mode: str = 'train',\n",
    "        window_stride: int = 15  # How many frames to advance for each new window\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            json_data: List of dictionaries containing video data (for reference only)\n",
    "            videos_dir: Directory containing the video files\n",
    "            max_frames: Number of frames in each sliding window\n",
    "            frame_size: Size to resize frames to\n",
    "            transform: Additional transforms to apply to frames\n",
    "            mode: Dataset mode ('train', 'val', or 'test') for file mapping\n",
    "            window_stride: Number of frames to advance for each new window\n",
    "        \"\"\"\n",
    "        self.json_data = json_data\n",
    "        self.videos_dir = videos_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.mode = mode\n",
    "        self.window_stride = window_stride\n",
    "        \n",
    "        # Create default transform if none provided\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "        # Generate samples based on the video file\n",
    "        self.video_path, self.effect_map = self._get_video_info()\n",
    "        self.samples = self._generate_samples()\n",
    "        \n",
    "    def _get_video_info(self) -> Tuple[str, Dict[int, int]]:\n",
    "        \"\"\"\n",
    "        Get video path and create a mapping of frame numbers to effects\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (video_path, effect_map)\n",
    "        \"\"\"\n",
    "        # Map the filename to the correct video file based on mode\n",
    "        if self.mode == 'train':\n",
    "            video_filename = 'train_set.mp4'\n",
    "        elif self.mode == 'val':\n",
    "            video_filename = 'val_set.mp4'\n",
    "        elif self.mode == 'test':\n",
    "            video_filename = 'test_set.mp4'\n",
    "        else:\n",
    "            # Use the filename from the first JSON entry as fallback\n",
    "            video_filename = self.json_data[0][\"filename\"]\n",
    "            \n",
    "        video_path = os.path.join(self.videos_dir, video_filename)\n",
    "        \n",
    "        # Create a mapping of frame numbers to effect labels\n",
    "        effect_map = {}\n",
    "        \n",
    "        # Go through all segments in JSON data and mark the effect labels\n",
    "        for video_data in self.json_data:\n",
    "            for segment in video_data[\"segments\"]:\n",
    "                start_frame = segment[\"start_frame\"]\n",
    "                end_frame = segment[\"end_frame\"]\n",
    "                effect = segment[\"effect\"]\n",
    "                \n",
    "                # Mark every frame in this segment with the effect label\n",
    "                for frame_idx in range(start_frame, end_frame):\n",
    "                    effect_map[frame_idx] = effect\n",
    "        \n",
    "        return video_path, effect_map\n",
    "        \n",
    "    def _generate_samples(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate a list of sliding windows from the video\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing window info\n",
    "        \"\"\"\n",
    "        # Open video to get properties\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "        \n",
    "        # Get video total frame count\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        # Create sliding windows over the entire video\n",
    "        for start_idx in range(0, total_frames - self.max_frames + 1, self.window_stride):\n",
    "            end_idx = start_idx + self.max_frames\n",
    "            \n",
    "            # Determine the effect label for this window based on majority vote\n",
    "            frame_labels = [self.effect_map.get(i, 0) for i in range(start_idx, end_idx)]\n",
    "            effect = 1 if sum(frame_labels) > len(frame_labels) / 3 else 0\n",
    "            \n",
    "            sample_info = {\n",
    "                \"start_frame\": start_idx,\n",
    "                \"end_frame\": end_idx,\n",
    "                \"effect\": effect\n",
    "            }\n",
    "            \n",
    "            samples.append(sample_info)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _extract_frames(self, start_frame: int, end_frame: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract frames from a video segment\n",
    "        \n",
    "        Args:\n",
    "            start_frame: Start frame index\n",
    "            end_frame: End frame index\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of frames\n",
    "        \"\"\"\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "        \n",
    "        # Extract consecutive frames (we want exactly max_frames)\n",
    "        frames = []\n",
    "        for frame_idx in range(start_frame, end_frame):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, self.frame_size)\n",
    "            \n",
    "            # Convert from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # If no frames were extracted or fewer than expected\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"No frames extracted from {self.video_path}\")\n",
    "        elif len(frames) < self.max_frames:\n",
    "            # Pad with the last frame\n",
    "            last_frame = frames[-1]\n",
    "            frames.extend([last_frame] * (self.max_frames - len(frames)))\n",
    "        \n",
    "        return np.array(frames)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples (windows) in the dataset\"\"\"\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (frames, label)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Extract frames\n",
    "            frames = self._extract_frames(\n",
    "                sample[\"start_frame\"],\n",
    "                sample[\"end_frame\"]\n",
    "            )\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                frames = frames[::-1]\n",
    "            \n",
    "            # Apply transforms to each frame\n",
    "            transformed_frames = []\n",
    "            for frame in frames:\n",
    "                transformed_frame = self.transform(frame)\n",
    "                transformed_frames.append(transformed_frame)\n",
    "            \n",
    "            # Stack frames\n",
    "            frames_tensor = torch.stack(transformed_frames)\n",
    "            \n",
    "            # Get label\n",
    "            label = sample[\"effect\"]\n",
    "            \n",
    "            return frames_tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return a default/placeholder\n",
    "            # Create a tensor of zeros with the expected shape\n",
    "            empty_frames = torch.zeros(self.max_frames, 3, self.frame_size[0], self.frame_size[1])\n",
    "            # Return default label as 0\n",
    "            return empty_frames, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef6e933-25b7-48b7-acff-a78210ec2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_json_path: str,\n",
    "    val_json_path: str,\n",
    "    test_json_path: str,\n",
    "    videos_dir: str = VIDEOS_DIR,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_frames: int = MAX_FRAMES,\n",
    "    frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "    num_workers: int = NUM_WORKERS,\n",
    "    window_stride: int = 30  # Added parameter\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders from separate JSON files\n",
    "    \n",
    "    Args:\n",
    "        train_json_path: Path to training data JSON\n",
    "        val_json_path: Path to validation data JSON\n",
    "        test_json_path: Path to test data JSON\n",
    "        videos_dir: Directory containing the video files\n",
    "        batch_size: Batch size for dataloaders\n",
    "        max_frames: Maximum number of frames in each sliding window\n",
    "        frame_size: Size to resize frames to\n",
    "        num_workers: Number of worker processes for dataloaders\n",
    "        window_stride: Number of frames to advance for each new window\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    with open(train_json_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    with open(val_json_path, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    with open(test_json_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        json_data=train_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='train',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    val_dataset = VideoDataset(\n",
    "        json_data=val_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='val',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    test_dataset = VideoDataset(\n",
    "        json_data=test_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='test',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0aadf2-570e-4c13-b703-7766a7b4e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1282\n",
      "Validation dataset size: 273\n",
      "Test dataset size: 273\n",
      "Batch shape: torch.Size([4, 30, 3, 224, 224])\n",
      "Labels: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders from separate JSON files\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR\n",
    "    )\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    # Test a batch\n",
    "    for frames, labels in train_loader:\n",
    "        print(f\"Batch shape: {frames.shape}\")  # Should be [batch_size, max_frames, 3, height, width]\n",
    "        print(f\"Labels: {labels}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1674ea3e-e903-4fac-bcb4-fe27da962a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(FrameFeatureExtractor, self).__init__()\n",
    "        # Use MobileNetV3-Small as the feature extractor (lightweight)\n",
    "        # UPDATED: T Efficent-Net\n",
    "        base_model = models.efficientnet_b0(weights='DEFAULT' if pretrained else None)\n",
    "        # Remove the classifier layer\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        # MobileNetV3-Small outputs 576-dimensional features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # UDPATED: From 576 to 1280 (EfficientNet-B0 output size)\n",
    "        self.fc = nn.Linear(1280, FEATURE_DIM)  # was nn.Linear(576, FEATURE_DIM)\n",
    "        # self.fc = nn.Linear(576, FEATURE_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size * num_frames, channels, height, width)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af8f80b-b80b-4ad1-b3aa-98979db86d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.feature_extractor = FrameFeatureExtractor()\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, MAX_FRAMES, FEATURE_DIM) * 0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=FEATURE_DIM,\n",
    "            nhead=NUM_HEADS,\n",
    "            dim_feedforward=FEATURE_DIM * 4,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(FEATURE_DIM, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "\n",
    "        # Reshape for feature extraction\n",
    "        x = x.view(batch_size * num_frames, channels, height, width)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Reshape back to (batch_size, num_frames, feature_dim)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        # Add positional encoding\n",
    "        features = features + self.pos_encoder\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "\n",
    "        # Global pooling over sequence dimension\n",
    "        pooled_output = torch.mean(transformer_output, dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16038afe-69dd-40ec-bae0-35c915efce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        frames, labels = batch\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate predictions\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return epoch_loss / len(dataloader), accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            frames, labels = batch\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return val_loss / len(dataloader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16c1fd69-ebaf-4cc2-964b-b42fd51be1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_values, val_values, metric_name):\n",
    "    \"\"\"\n",
    "    Plot the training and validation metrics\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_values, label=f'Train {metric_name}')\n",
    "    plt.plot(val_values, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{metric_name.lower().replace(\" \", \"_\")}_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12730d5d-1b54-4836-a897-11da9af414e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Create data loaders\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(\"Preparing data...\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    print(\"Data loaded!\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Create model\n",
    "    print(\"Initializing model...\")\n",
    "    model = TransformerClassifier().to(DEVICE)\n",
    "\n",
    "    # Print model summary\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_model_path = 'best_effect_detection_model.pth'\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, DEVICE\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model (using F1 score as the metric)\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation F1: {val_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"Plotting metrics...\")\n",
    "    plot_metrics(train_losses, val_losses, 'Loss')\n",
    "    plot_metrics(train_accuracies, val_accuracies, 'Accuracy')\n",
    "    plot_metrics(train_f1s, val_f1s, 'F1 Score')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = validate(\n",
    "        model, test_loader, criterion, DEVICE\n",
    "    )\n",
    "\n",
    "    print(f\"Test Results - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52553a8c-2131-4627-a64d-a9261a45686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on single sequences\n",
    "def evaluate_sequence(model, video_path, start_frame, num_frames, transform):\n",
    "    \"\"\"\n",
    "    Evaluate a single video sequence for effect detection\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame + i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply transform\n",
    "        frame = cv2.resize(frame, FRAME_SIZE)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if transform:\n",
    "            frame = transform(frame)\n",
    "        else:\n",
    "            # Convert to tensor manually\n",
    "            frame = torch.FloatTensor(frame / 255.0).permute(2, 0, 1)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Stack frames\n",
    "    if len(frames) < num_frames:\n",
    "        # Pad with zeros if not enough frames\n",
    "        for _ in range(num_frames - len(frames)):\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames_tensor.to(DEVICE))\n",
    "        probability = torch.sigmoid(outputs).item()\n",
    "        prediction = 1 if probability > 0.5 else 0\n",
    "\n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e54a652-75fb-4efe-afcf-4f32e15d9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaan-eren/jupyter_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing data...\n",
      "Data loaded!\n",
      "Train batches: 321\n",
      "Val batches: 69\n",
      "Test batches: 69\n",
      "Initializing model...\n",
      "Model initialized with 6,794,749 trainable parameters\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/321 [00:00<?, ?it/s]/tmp/ipykernel_1909139/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|████████████| 321/321 [03:44<00:00,  1.43it/s, batch_loss=0.2251]\n",
      "Validation: 100%|███████████████████████████████| 69/69 [00:51<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.6028, Acc: 0.6864, Prec: 0.6820, Rec: 0.9663, F1: 0.7996\n",
      "Val   - Loss: 0.5068, Acc: 0.7363, Prec: 0.7490, Rec: 0.9430, F1: 0.8349\n",
      "New best model saved with validation F1: 0.8349\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/321 [00:00<?, ?it/s]/tmp/ipykernel_1909139/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|████████████| 321/321 [03:47<00:00,  1.41it/s, batch_loss=0.2694]\n",
      "Validation: 100%|███████████████████████████████| 69/69 [00:52<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.4471, Acc: 0.8276, Prec: 0.8342, Rec: 0.9157, F1: 0.8731\n",
      "Val   - Loss: 0.4271, Acc: 0.8315, Prec: 0.8267, Rec: 0.9637, F1: 0.8900\n",
      "New best model saved with validation F1: 0.8900\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/321 [00:00<?, ?it/s]/tmp/ipykernel_1909139/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|████████████| 321/321 [03:50<00:00,  1.39it/s, batch_loss=0.0344]\n",
      "Validation: 100%|███████████████████████████████| 69/69 [00:52<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.3361, Acc: 0.8830, Prec: 0.9038, Rec: 0.9169, F1: 0.9103\n",
      "Val   - Loss: 0.5548, Acc: 0.8132, Prec: 0.8227, Rec: 0.9378, F1: 0.8765\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/321 [00:00<?, ?it/s]/tmp/ipykernel_1909139/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|████████████| 321/321 [03:50<00:00,  1.39it/s, batch_loss=1.4003]\n",
      "Validation: 100%|███████████████████████████████| 69/69 [00:52<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.3783, Acc: 0.8861, Prec: 0.9062, Rec: 0.9193, F1: 0.9127\n",
      "Val   - Loss: 0.5252, Acc: 0.8462, Prec: 0.8479, Rec: 0.9534, F1: 0.8976\n",
      "New best model saved with validation F1: 0.8976\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/321 [00:00<?, ?it/s]/tmp/ipykernel_1909139/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|████████████| 321/321 [03:50<00:00,  1.39it/s, batch_loss=0.0933]\n",
      "Validation: 100%|███████████████████████████████| 69/69 [00:52<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.3436, Acc: 0.9041, Prec: 0.9274, Rec: 0.9241, F1: 0.9258\n",
      "Val   - Loss: 0.5893, Acc: 0.8498, Prec: 0.8333, Rec: 0.9845, F1: 0.9026\n",
      "New best model saved with validation F1: 0.9026\n",
      "\n",
      "Training completed!\n",
      "Plotting metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1909139/785972949.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████| 69/69 [00:48<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results - Loss: 0.7138, Acc: 0.8278, Prec: 0.8846, Rec: 0.8889, F1: 0.8867\n",
      "Final model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model = train_model()\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), 'final_effect_detection_model.pth')\n",
    "    print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a269b5a9-b341-4ee6-af46-93ea86c27517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Total Memory: 7.63 GB\n",
      "GPU Memory Allocated: 0.07 GB\n",
      "GPU Memory Reserved: 6.83 GB\n",
      "GPU Memory Free: 0.64 GB\n"
     ]
    }
   ],
   "source": [
    "# Check total GPU memory\n",
    "print(f\"GPU Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check current memory usage\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check free memory\n",
    "free_memory, total_memory = torch.cuda.mem_get_info(0)\n",
    "print(f\"GPU Memory Free: {free_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28aa5e-e0e3-4aaa-8527-185018554ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11657ac-4524-4d83-a510-bb5379956252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
