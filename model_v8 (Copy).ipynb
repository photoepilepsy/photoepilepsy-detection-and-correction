{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a48d22-54f2-46e4-a21f-182e3ed2f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlabelling upd\\npos encoder upd\\nlr dec\\ndropout inv\\n\\n\\nif random.random() > 0.5:\\n    frames = frames[::-1]  # Reverse sequence\\n\\nFEATURE_DIM = 256 \\nlr inc\\n\\nfut:\\ndecrase lr \\ninc epoch\\n\\nhand tailed labelas\\n\\nsmooth labels can be used\\n\\nlstm exper\\nmin exp\\n\\n2x more data.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "labelling upd\n",
    "pos encoder upd\n",
    "lr dec\n",
    "dropout inv\n",
    "\n",
    "\n",
    "if random.random() > 0.5:\n",
    "    frames = frames[::-1]  # Reverse sequence\n",
    "\n",
    "FEATURE_DIM = 256 \n",
    "lr inc\n",
    "\n",
    "fut:\n",
    "decrase lr \n",
    "inc epoch\n",
    "\n",
    "hand tailed labelas\n",
    "\n",
    "smooth labels can be used\n",
    "\n",
    "lstm exper\n",
    "min exp\n",
    "\n",
    "2x more data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f857c6-4294-45e5-94da-80b6cfa3785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a30700-65a2-4b5a-89bf-057423db75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "VIDEOS_DIR = '/home/kaan-eren/projects/V4_BITIRME'\n",
    "MAX_FRAMES = 30  # Frame window size\n",
    "FRAME_SIZE = (224, 224)  # Standard size for most pre-trained models\n",
    "BATCH_SIZE = 4  # Adjust based on VRAM\n",
    "NUM_WORKERS = 1  \n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 4e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "FEATURE_DIM = 1024  # Dimensionality of frame features\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "NUM_LAYERS = 3  # Number of transformer layers\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9d629-9c4c-4523-a01f-7a32e5d3a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for video frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(FRAME_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76752a46-554a-4b88-8e70-8fec0ebff07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle video data for effect detection model\n",
    "    \n",
    "    This dataset processes an entire video file and extracts frames at regular intervals,\n",
    "    creating a dataset of consecutive frame windows for training the effect detection model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        json_data: List[Dict], \n",
    "        videos_dir: str,\n",
    "        max_frames: int = MAX_FRAMES,\n",
    "        frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "        transform=None,\n",
    "        mode: str = 'train',\n",
    "        window_stride: int = 15  # How many frames to advance for each new window\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            json_data: List of dictionaries containing video data (for reference only)\n",
    "            videos_dir: Directory containing the video files\n",
    "            max_frames: Number of frames in each sliding window\n",
    "            frame_size: Size to resize frames to\n",
    "            transform: Additional transforms to apply to frames\n",
    "            mode: Dataset mode ('train', 'val', or 'test') for file mapping\n",
    "            window_stride: Number of frames to advance for each new window\n",
    "        \"\"\"\n",
    "        self.json_data = json_data\n",
    "        self.videos_dir = videos_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.mode = mode\n",
    "        self.window_stride = window_stride\n",
    "        \n",
    "        # Create default transform if none provided\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "        # Generate samples based on the video file\n",
    "        self.video_path, self.effect_map = self._get_video_info()\n",
    "        self.samples = self._generate_samples()\n",
    "        \n",
    "    def _get_video_info(self) -> Tuple[str, Dict[int, int]]:\n",
    "        \"\"\"\n",
    "        Get video path and create a mapping of frame numbers to effects\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (video_path, effect_map)\n",
    "        \"\"\"\n",
    "        # Map the filename to the correct video file based on mode\n",
    "        if self.mode == 'train':\n",
    "            video_filename = 'train_set.mp4'\n",
    "        elif self.mode == 'val':\n",
    "            video_filename = 'val_set.mp4'\n",
    "        elif self.mode == 'test':\n",
    "            video_filename = 'test_set.mp4'\n",
    "        else:\n",
    "            # Use the filename from the first JSON entry as fallback\n",
    "            video_filename = self.json_data[0][\"filename\"]\n",
    "            \n",
    "        video_path = os.path.join(self.videos_dir, video_filename)\n",
    "        \n",
    "        # Create a mapping of frame numbers to effect labels\n",
    "        effect_map = {}\n",
    "        \n",
    "        # Go through all segments in JSON data and mark the effect labels\n",
    "        for video_data in self.json_data:\n",
    "            for segment in video_data[\"segments\"]:\n",
    "                start_frame = segment[\"start_frame\"]\n",
    "                end_frame = segment[\"end_frame\"]\n",
    "                effect = segment[\"effect\"]\n",
    "                \n",
    "                # Mark every frame in this segment with the effect label\n",
    "                for frame_idx in range(start_frame, end_frame):\n",
    "                    effect_map[frame_idx] = effect\n",
    "        \n",
    "        return video_path, effect_map\n",
    "        \n",
    "    def _generate_samples(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate a list of sliding windows from the video\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing window info\n",
    "        \"\"\"\n",
    "        # Open video to get properties\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "        \n",
    "        # Get video total frame count\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        # Create sliding windows over the entire video\n",
    "        for start_idx in range(0, total_frames - self.max_frames + 1, self.window_stride):\n",
    "            end_idx = start_idx + self.max_frames\n",
    "            \n",
    "            # Determine the effect label for this window based on majority vote\n",
    "            frame_labels = [self.effect_map.get(i, 0) for i in range(start_idx, end_idx)]\n",
    "            effect = 1 if sum(frame_labels) > len(frame_labels) / 3 else 0\n",
    "            \n",
    "            sample_info = {\n",
    "                \"start_frame\": start_idx,\n",
    "                \"end_frame\": end_idx,\n",
    "                \"effect\": effect\n",
    "            }\n",
    "            \n",
    "            samples.append(sample_info)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _extract_frames(self, start_frame: int, end_frame: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract frames from a video segment\n",
    "        \n",
    "        Args:\n",
    "            start_frame: Start frame index\n",
    "            end_frame: End frame index\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of frames\n",
    "        \"\"\"\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "        \n",
    "        # Extract consecutive frames (we want exactly max_frames)\n",
    "        frames = []\n",
    "        for frame_idx in range(start_frame, end_frame):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, self.frame_size)\n",
    "            \n",
    "            # Convert from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # If no frames were extracted or fewer than expected\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"No frames extracted from {self.video_path}\")\n",
    "        elif len(frames) < self.max_frames:\n",
    "            # Pad with the last frame\n",
    "            last_frame = frames[-1]\n",
    "            frames.extend([last_frame] * (self.max_frames - len(frames)))\n",
    "        \n",
    "        return np.array(frames)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples (windows) in the dataset\"\"\"\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (frames, label)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Extract frames\n",
    "            frames = self._extract_frames(\n",
    "                sample[\"start_frame\"],\n",
    "                sample[\"end_frame\"]\n",
    "            )\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                frames = frames[::-1]\n",
    "            \n",
    "            # Apply transforms to each frame\n",
    "            transformed_frames = []\n",
    "            for frame in frames:\n",
    "                transformed_frame = self.transform(frame)\n",
    "                transformed_frames.append(transformed_frame)\n",
    "            \n",
    "            # Stack frames\n",
    "            frames_tensor = torch.stack(transformed_frames)\n",
    "            \n",
    "            # Get label\n",
    "            label = sample[\"effect\"]\n",
    "            \n",
    "            return frames_tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return a default/placeholder\n",
    "            # Create a tensor of zeros with the expected shape\n",
    "            empty_frames = torch.zeros(self.max_frames, 3, self.frame_size[0], self.frame_size[1])\n",
    "            # Return default label as 0\n",
    "            return empty_frames, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef6e933-25b7-48b7-acff-a78210ec2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_json_path: str,\n",
    "    val_json_path: str,\n",
    "    test_json_path: str,\n",
    "    videos_dir: str = VIDEOS_DIR,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_frames: int = MAX_FRAMES,\n",
    "    frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "    num_workers: int = NUM_WORKERS,\n",
    "    window_stride: int = 30  # Added parameter\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders from separate JSON files\n",
    "    \n",
    "    Args:\n",
    "        train_json_path: Path to training data JSON\n",
    "        val_json_path: Path to validation data JSON\n",
    "        test_json_path: Path to test data JSON\n",
    "        videos_dir: Directory containing the video files\n",
    "        batch_size: Batch size for dataloaders\n",
    "        max_frames: Maximum number of frames in each sliding window\n",
    "        frame_size: Size to resize frames to\n",
    "        num_workers: Number of worker processes for dataloaders\n",
    "        window_stride: Number of frames to advance for each new window\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    with open(train_json_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    with open(val_json_path, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    with open(test_json_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        json_data=train_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='train',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    val_dataset = VideoDataset(\n",
    "        json_data=val_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='val',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    test_dataset = VideoDataset(\n",
    "        json_data=test_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='test',\n",
    "        window_stride=window_stride  # Added parameter\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0aadf2-570e-4c13-b703-7766a7b4e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1282\n",
      "Validation dataset size: 273\n",
      "Test dataset size: 273\n",
      "Batch shape: torch.Size([4, 30, 3, 224, 224])\n",
      "Labels: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders from separate JSON files\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR\n",
    "    )\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    # Test a batch\n",
    "    for frames, labels in train_loader:\n",
    "        print(f\"Batch shape: {frames.shape}\")  # Should be [batch_size, max_frames, 3, height, width]\n",
    "        print(f\"Labels: {labels}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1674ea3e-e903-4fac-bcb4-fe27da962a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(FrameFeatureExtractor, self).__init__()\n",
    "        # Use MobileNetV3-Small as the feature extractor (lightweight)\n",
    "        # UPDATED: T Efficent-Net\n",
    "        base_model = models.efficientnet_b0(weights='DEFAULT' if pretrained else None)\n",
    "        # Remove the classifier layer\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        # MobileNetV3-Small outputs 576-dimensional features\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # UDPATED: From 576 to 1280 (EfficientNet-B0 output size)\n",
    "        self.fc = nn.Linear(1280, FEATURE_DIM)  # was nn.Linear(576, FEATURE_DIM)\n",
    "        # self.fc = nn.Linear(576, FEATURE_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size * num_frames, channels, height, width)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af8f80b-b80b-4ad1-b3aa-98979db86d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.feature_extractor = FrameFeatureExtractor()\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, MAX_FRAMES, FEATURE_DIM) * 0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=FEATURE_DIM,\n",
    "            nhead=NUM_HEADS,\n",
    "            dim_feedforward=FEATURE_DIM * 4,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(FEATURE_DIM, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "\n",
    "        # Reshape for feature extraction\n",
    "        x = x.view(batch_size * num_frames, channels, height, width)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Reshape back to (batch_size, num_frames, feature_dim)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        # Add positional encoding\n",
    "        features = features + self.pos_encoder\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "\n",
    "        # Global pooling over sequence dimension\n",
    "        pooled_output = torch.mean(transformer_output, dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16038afe-69dd-40ec-bae0-35c915efce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        frames, labels = batch\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate predictions\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return epoch_loss / len(dataloader), accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            frames, labels = batch\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return val_loss / len(dataloader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16c1fd69-ebaf-4cc2-964b-b42fd51be1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_values, val_values, metric_name):\n",
    "    \"\"\"\n",
    "    Plot the training and validation metrics\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_values, label=f'Train {metric_name}')\n",
    "    plt.plot(val_values, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{metric_name.lower().replace(\" \", \"_\")}_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12730d5d-1b54-4836-a897-11da9af414e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Create data loaders\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(\"Preparing data...\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        window_stride=15  # Reduced from default 30 to create more training samples\n",
    "    )\n",
    "\n",
    "    print(\"Data loaded!\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Create model\n",
    "    print(\"Initializing model...\")\n",
    "    model = TransformerClassifier().to(DEVICE)\n",
    "\n",
    "    # Print model summary\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "    # Loss function with positive weight to prioritize recall\n",
    "    pos_weight = torch.tensor([2.0]).to(DEVICE)  # Increased weight for positive class\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler monitoring recall instead of loss\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, verbose=True, min_lr=5e-7\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_recalls, val_recalls = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    # Track best metrics with weighted combination\n",
    "    best_combined_metric = 0\n",
    "    best_val_recall = 0\n",
    "    best_model_path = 'best_effect_detection_model.pth'\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, DEVICE\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "\n",
    "        # Calculate weighted metric - prioritize recall (0.6) while maintaining accuracy (0.4)\n",
    "        combined_metric = 0.6 * val_rec + 0.4 * val_acc\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_recalls.append(train_rec)\n",
    "        val_recalls.append(val_rec)\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(f\"Combined Metric (0.6*Recall + 0.4*Accuracy): {combined_metric:.4f}\")\n",
    "\n",
    "        # Update learning rate based on recall\n",
    "        scheduler.step(val_rec)\n",
    "\n",
    "        # Save best model based on combined metric\n",
    "        if combined_metric > best_combined_metric:\n",
    "            best_combined_metric = combined_metric\n",
    "            best_val_recall = val_rec\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with val recall: {val_rec:.4f}, val accuracy: {val_acc:.4f}, combined metric: {combined_metric:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"Plotting metrics...\")\n",
    "    plot_metrics(train_losses, val_losses, 'Loss')\n",
    "    plot_metrics(train_accuracies, val_accuracies, 'Accuracy')\n",
    "    plot_metrics(train_recalls, val_recalls, 'Recall')\n",
    "    plot_metrics(train_f1s, val_f1s, 'F1 Score')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    # For test evaluation, use a lower threshold to further boost recall\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = validate_with_threshold(\n",
    "        model, test_loader, criterion, DEVICE, threshold=0.4  # Lower threshold for test set\n",
    "    )\n",
    "\n",
    "    print(f\"Test Results - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Add a modified validation function that allows for threshold adjustment\n",
    "def validate_with_threshold(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            frames, labels = batch\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Use custom threshold\n",
    "            preds = (torch.sigmoid(outputs) > threshold).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return val_loss / len(dataloader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52553a8c-2131-4627-a64d-a9261a45686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on single sequences\n",
    "def evaluate_sequence(model, video_path, start_frame, num_frames, transform):\n",
    "    \"\"\"\n",
    "    Evaluate a single video sequence for effect detection\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame + i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply transform\n",
    "        frame = cv2.resize(frame, FRAME_SIZE)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if transform:\n",
    "            frame = transform(frame)\n",
    "        else:\n",
    "            # Convert to tensor manually\n",
    "            frame = torch.FloatTensor(frame / 255.0).permute(2, 0, 1)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Stack frames\n",
    "    if len(frames) < num_frames:\n",
    "        # Pad with zeros if not enough frames\n",
    "        for _ in range(num_frames - len(frames)):\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames_tensor.to(DEVICE))\n",
    "        probability = torch.sigmoid(outputs).item()\n",
    "        prediction = 1 if probability > 0.5 else 0\n",
    "\n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e54a652-75fb-4efe-afcf-4f32e15d9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaan-eren/jupyter_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing data...\n",
      "Data loaded!\n",
      "Train batches: 641\n",
      "Val batches: 137\n",
      "Test batches: 137\n",
      "Initializing model...\n",
      "Model initialized with 43,417,597 trainable parameters\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/641 [00:00<?, ?it/s]/tmp/ipykernel_2505881/2964570289.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training:  21%|██▌         | 134/641 [01:37<06:10,  1.37it/s, batch_loss=1.0984]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_effect_detection_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m train_loss, train_acc, train_prec, train_rec, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m     66\u001b[0m val_loss, val_acc, val_prec, val_rec, val_f1 \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m     67\u001b[0m     model, val_loader, criterion, DEVICE\n\u001b[1;32m     68\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model = train_model()\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), 'final_effect_detection_model.pth')\n",
    "    print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269b5a9-b341-4ee6-af46-93ea86c27517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total GPU memory\n",
    "print(f\"GPU Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check current memory usage\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check free memory\n",
    "free_memory, total_memory = torch.cuda.mem_get_info(0)\n",
    "print(f\"GPU Memory Free: {free_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28aa5e-e0e3-4aaa-8527-185018554ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11657ac-4524-4d83-a510-bb5379956252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
