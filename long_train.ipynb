{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a201c8-2269-4aa4-badb-7ed7a1816390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training set: 1406 positive, 1406 negative\n",
      "Train dataset size: 2812\n",
      "Validation dataset size: 819\n",
      "Test dataset size: 819\n",
      "Batch shape: torch.Size([4, 30, 3, 224, 224])\n",
      "Labels: tensor([0, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2304036/3715828582.py:680: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing data...\n",
      "Balanced training set: 1406 positive, 1406 negative\n",
      "Data loaded!\n",
      "Train batches: 703\n",
      "Val batches: 205\n",
      "Test batches: 205\n",
      "Initializing model...\n",
      "Model initialized with 7,454,205 trainable parameters\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████| 703/703 [08:16<00:00,  1.42it/s, batch_loss=0.1862]\n",
      "Validation: 100%|█████████████████████████████| 205/205 [02:33<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1744, Acc: 0.5661, Prec: 0.5819, Rec: 0.4701, F1: 0.5201\n",
      "Val   - Loss: 0.1466, Acc: 0.6642, Prec: 0.7289, Rec: 0.8359, F1: 0.7788\n",
      "LR: 0.000027\n",
      "New best model saved with validation F1: 0.7788\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▌         | 153/703 [01:47<06:25,  1.43it/s, batch_loss=0.1452]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 806\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, probability\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_effect_detection_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 700\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# UPDATED: Train with gradient scaler\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m train_loss, train_acc, train_prec, train_rec, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    705\u001b[0m val_loss, val_acc, val_prec, val_rec, val_f1 \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m    706\u001b[0m     model, val_loader, criterion, DEVICE\n\u001b[1;32m    707\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 557\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, scaler)\u001b[0m\n\u001b[1;32m    554\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    556\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 557\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"model_v8 best.ipynb - UPDATED VERSION\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1hLEuxwylY6keQUVYxoMtu24PYsojRtiD\n",
    "\n",
    "labelling upd\n",
    "pos encoder upd\n",
    "lr dec\n",
    "dropout inv\n",
    "\n",
    "\n",
    "if random.random() > 0.5:\n",
    "    frames = frames[::-1]  # Reverse sequence\n",
    "\n",
    "FEATURE_DIM = 256\n",
    "lr inc\n",
    "\n",
    "fut:\n",
    "decrase lr\n",
    "inc epoch\n",
    "\n",
    "hand tailed labelas\n",
    "\n",
    "smooth labels can be used\n",
    "\n",
    "lstm exper\n",
    "min exp\n",
    "\n",
    "2x more data.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "# Global parameters\n",
    "VIDEOS_DIR = '/home/kaan-eren/projects/V4_BITIRME'\n",
    "MAX_FRAMES = 30  # Frame window size\n",
    "FRAME_SIZE = (224, 224)  # Standard size for most pre-trained models\n",
    "# UPDATED: Reduced batch size for stability with larger model\n",
    "BATCH_SIZE = 4  # Reduced from 4 for better gradient stability\n",
    "NUM_WORKERS = 1\n",
    "# UPDATED: Increased epochs with early stopping\n",
    "NUM_EPOCHS = 5  # Increased from 5\n",
    "# UPDATED: Lower learning rate to prevent overfitting\n",
    "LEARNING_RATE = 3e-5  # Reduced from 3e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# UPDATED: Reduced feature dimension for better regularization\n",
    "FEATURE_DIM = 256  # Reduced from 1024\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "# UPDATED: Increased transformer layers\n",
    "NUM_LAYERS = 3 # Increased from 3\n",
    "# UPDATED: Increased dropout for better regularization\n",
    "DROPOUT = 0.4  # Increased from 0.3\n",
    "\n",
    "# Transformations for video frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(FRAME_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to handle video data for effect detection model\n",
    "\n",
    "    This dataset processes an entire video file and extracts frames at regular intervals,\n",
    "    creating a dataset of consecutive frame windows for training the effect detection model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_data: List[Dict],\n",
    "        videos_dir: str,\n",
    "        max_frames: int = MAX_FRAMES,\n",
    "        frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "        transform=None,\n",
    "        mode: str = 'train',\n",
    "        # UPDATED: Reduced window stride for more training data\n",
    "        window_stride: int = 10  # Reduced from 15 for more overlapping windows\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "\n",
    "        Args:\n",
    "            json_data: List of dictionaries containing video data (for reference only)\n",
    "            videos_dir: Directory containing the video files\n",
    "            max_frames: Number of frames in each sliding window\n",
    "            frame_size: Size to resize frames to\n",
    "            transform: Additional transforms to apply to frames\n",
    "            mode: Dataset mode ('train', 'val', or 'test') for file mapping\n",
    "            window_stride: Number of frames to advance for each new window\n",
    "        \"\"\"\n",
    "        self.json_data = json_data\n",
    "        self.videos_dir = videos_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.mode = mode\n",
    "        self.window_stride = window_stride\n",
    "\n",
    "        # Create default transform if none provided\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        # Generate samples based on the video file\n",
    "        self.video_path, self.effect_map = self._get_video_info()\n",
    "        # UPDATED: Use improved sample generation\n",
    "        self.samples = self._generate_improved_samples()  # Changed method name\n",
    "\n",
    "    def _get_video_info(self) -> Tuple[str, Dict[int, int]]:\n",
    "        \"\"\"\n",
    "        Get video path and create a mapping of frame numbers to effects\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (video_path, effect_map)\n",
    "        \"\"\"\n",
    "        # Map the filename to the correct video file based on mode\n",
    "        if self.mode == 'train':\n",
    "            video_filename = 'train_set.mp4'\n",
    "        elif self.mode == 'val':\n",
    "            video_filename = 'val_set.mp4'\n",
    "        elif self.mode == 'test':\n",
    "            video_filename = 'test_set.mp4'\n",
    "        else:\n",
    "            # Use the filename from the first JSON entry as fallback\n",
    "            video_filename = self.json_data[0][\"filename\"]\n",
    "\n",
    "        video_path = os.path.join(self.videos_dir, video_filename)\n",
    "\n",
    "        # Create a mapping of frame numbers to effect labels\n",
    "        effect_map = {}\n",
    "\n",
    "        # Go through all segments in JSON data and mark the effect labels\n",
    "        for video_data in self.json_data:\n",
    "            for segment in video_data[\"segments\"]:\n",
    "                start_frame = segment[\"start_frame\"]\n",
    "                end_frame = segment[\"end_frame\"]\n",
    "                effect = segment[\"effect\"]\n",
    "\n",
    "                # Mark every frame in this segment with the effect label\n",
    "                for frame_idx in range(start_frame, end_frame):\n",
    "                    effect_map[frame_idx] = effect\n",
    "\n",
    "        return video_path, effect_map\n",
    "\n",
    "    # UPDATED: Improved sample generation method\n",
    "    def _generate_improved_samples(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate a list of sliding windows from the video with better labeling strategy\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing window info\n",
    "        \"\"\"\n",
    "        # Open video to get properties\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "\n",
    "        # Get video total frame count\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "\n",
    "        samples = []\n",
    "        positive_samples = []\n",
    "        negative_samples = []\n",
    "\n",
    "        # Create sliding windows over the entire video\n",
    "        for start_idx in range(0, total_frames - self.max_frames + 1, self.window_stride):\n",
    "            end_idx = start_idx + self.max_frames\n",
    "\n",
    "            # UPDATED: Better labeling strategy using ratio instead of simple majority\n",
    "            frame_labels = [self.effect_map.get(i, 0) for i in range(start_idx, end_idx)]\n",
    "            effect_ratio = sum(frame_labels) / len(frame_labels)\n",
    "            \n",
    "            # UPDATED: Use threshold-based approach for more balanced dataset\n",
    "            effect = 1 if effect_ratio > 0.4 else 0  # Changed from majority vote (0.5) to 0.4\n",
    "\n",
    "            sample_info = {\n",
    "                \"start_frame\": start_idx,\n",
    "                \"end_frame\": end_idx,\n",
    "                \"effect\": effect,\n",
    "                # UPDATED: Store effect ratio for analysis\n",
    "                \"effect_ratio\": effect_ratio\n",
    "            }\n",
    "\n",
    "            # UPDATED: Separate positive and negative samples for balancing\n",
    "            if effect == 1:\n",
    "                positive_samples.append(sample_info)\n",
    "            else:\n",
    "                negative_samples.append(sample_info)\n",
    "\n",
    "        # UPDATED: Balance the dataset during training\n",
    "        if self.mode == 'train':\n",
    "            min_samples = min(len(positive_samples), len(negative_samples))\n",
    "            if min_samples > 0:\n",
    "                # Randomly sample to balance classes\n",
    "                positive_samples = random.sample(positive_samples, min_samples)\n",
    "                negative_samples = random.sample(negative_samples, min_samples)\n",
    "                print(f\"Balanced training set: {len(positive_samples)} positive, {len(negative_samples)} negative\")\n",
    "\n",
    "        samples = positive_samples + negative_samples\n",
    "        random.shuffle(samples)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _extract_frames(self, start_frame: int, end_frame: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract frames from a video segment\n",
    "\n",
    "        Args:\n",
    "            start_frame: Start frame index\n",
    "            end_frame: End frame index\n",
    "\n",
    "        Returns:\n",
    "            Numpy array of frames\n",
    "        \"\"\"\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "\n",
    "        # Extract consecutive frames (we want exactly max_frames)\n",
    "        frames = []\n",
    "        for frame_idx in range(start_frame, end_frame):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Resize frame\n",
    "            frame = cv2.resize(frame, self.frame_size)\n",
    "\n",
    "            # Convert from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # If no frames were extracted or fewer than expected\n",
    "        if len(frames) == 0:\n",
    "            raise ValueError(f\"No frames extracted from {self.video_path}\")\n",
    "        elif len(frames) < self.max_frames:\n",
    "            # Pad with the last frame\n",
    "            last_frame = frames[-1]\n",
    "            frames.extend([last_frame] * (self.max_frames - len(frames)))\n",
    "\n",
    "        return np.array(frames)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples (windows) in the dataset\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (frames, label)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            # Extract frames\n",
    "            frames = self._extract_frames(\n",
    "                sample[\"start_frame\"],\n",
    "                sample[\"end_frame\"]\n",
    "            )\n",
    "\n",
    "            # UPDATED: Reduced temporal augmentation probability for stability\n",
    "            if self.mode == 'train' and random.random() > 0.7:  # Changed from 0.5 to 0.7\n",
    "                frames = frames[::-1]\n",
    "\n",
    "            # Apply transforms to each frame\n",
    "            transformed_frames = []\n",
    "            for frame in frames:\n",
    "                transformed_frame = self.transform(frame)\n",
    "                transformed_frames.append(transformed_frame)\n",
    "\n",
    "            # Stack frames\n",
    "            frames_tensor = torch.stack(transformed_frames)\n",
    "\n",
    "            # Get label\n",
    "            label = sample[\"effect\"]\n",
    "\n",
    "            return frames_tensor, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return a default/placeholder\n",
    "            # Create a tensor of zeros with the expected shape\n",
    "            empty_frames = torch.zeros(self.max_frames, 3, self.frame_size[0], self.frame_size[1])\n",
    "            # Return default label as 0\n",
    "            return empty_frames, 0\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_json_path: str,\n",
    "    val_json_path: str,\n",
    "    test_json_path: str,\n",
    "    videos_dir: str = VIDEOS_DIR,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_frames: int = MAX_FRAMES,\n",
    "    frame_size: Tuple[int, int] = FRAME_SIZE,\n",
    "    num_workers: int = NUM_WORKERS,\n",
    "    # UPDATED: Reduced default window stride\n",
    "    window_stride: int = 10  # Reduced from 30\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders from separate JSON files\n",
    "\n",
    "    Args:\n",
    "        train_json_path: Path to training data JSON\n",
    "        val_json_path: Path to validation data JSON\n",
    "        test_json_path: Path to test data JSON\n",
    "        videos_dir: Directory containing the video files\n",
    "        batch_size: Batch size for dataloaders\n",
    "        max_frames: Maximum number of frames in each sliding window\n",
    "        frame_size: Size to resize frames to\n",
    "        num_workers: Number of worker processes for dataloaders\n",
    "        window_stride: Number of frames to advance for each new window\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    with open(train_json_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open(val_json_path, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    with open(test_json_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(\n",
    "        json_data=train_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='train',\n",
    "        window_stride=window_stride\n",
    "    )\n",
    "\n",
    "    val_dataset = VideoDataset(\n",
    "        json_data=val_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='val',\n",
    "        window_stride=window_stride\n",
    "    )\n",
    "\n",
    "    test_dataset = VideoDataset(\n",
    "        json_data=test_data,\n",
    "        videos_dir=videos_dir,\n",
    "        max_frames=max_frames,\n",
    "        frame_size=frame_size,\n",
    "        mode='test',\n",
    "        window_stride=window_stride\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders from separate JSON files\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR\n",
    "    )\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_loader.dataset)}\")\n",
    "\n",
    "    # Test a batch\n",
    "    for frames, labels in train_loader:\n",
    "        print(f\"Batch shape: {frames.shape}\")  # Should be [batch_size, max_frames, 3, height, width]\n",
    "        print(f\"Labels: {labels}\")\n",
    "        break\n",
    "\n",
    "class FrameFeatureExtractor(nn.Module):\n",
    "    # UPDATED: Enhanced feature extractor with better regularization\n",
    "    def __init__(self, pretrained=True, dropout=DROPOUT):\n",
    "        super(FrameFeatureExtractor, self).__init__()\n",
    "        # UPDATED: Use EfficientNet-B0 for better performance\n",
    "        base_model = models.efficientnet_b0(weights='DEFAULT' if pretrained else None)\n",
    "        # Remove the classifier layer\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # UPDATED: Enhanced feature projection with batch norm and dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            # UPDATED: EfficientNet-B1 outputs 1280 features\n",
    "            nn.Linear(1280, 512),  # Changed from direct mapping to FEATURE_DIM\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, FEATURE_DIM)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size * num_frames, channels, height, width)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # UPDATED: Use enhanced feature extractor\n",
    "        self.feature_extractor = FrameFeatureExtractor(dropout=DROPOUT)\n",
    "\n",
    "        # UPDATED: Improved positional encoding initialization\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, MAX_FRAMES, FEATURE_DIM) * 0.02)\n",
    "        # UPDATED: Add layer normalization and dropout for positional encoding\n",
    "        self.pos_dropout = nn.Dropout(DROPOUT)\n",
    "        self.layer_norm = nn.LayerNorm(FEATURE_DIM)\n",
    "\n",
    "        # UPDATED: Enhanced transformer encoder with GELU activation\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=FEATURE_DIM,\n",
    "            nhead=NUM_HEADS,\n",
    "            dim_feedforward=FEATURE_DIM * 4,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True,\n",
    "            activation='gelu'  # UPDATED: GELU instead of ReLU\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "\n",
    "        # UPDATED: Enhanced classification head with batch normalization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(FEATURE_DIM, 512),\n",
    "            nn.BatchNorm1d(512),  # UPDATED: Added batch norm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),  # UPDATED: Added batch norm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        batch_size, num_frames, channels, height, width = x.shape\n",
    "\n",
    "        # Reshape for feature extraction\n",
    "        x = x.view(batch_size * num_frames, channels, height, width)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Reshape back to (batch_size, num_frames, feature_dim)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        # UPDATED: Apply layer normalization before adding positional encoding\n",
    "        features = self.layer_norm(features + self.pos_encoder)\n",
    "        features = self.pos_dropout(features)\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "\n",
    "        # UPDATED: Use attention-weighted pooling instead of mean pooling\n",
    "        attention_weights = torch.softmax(\n",
    "            torch.mean(transformer_output, dim=-1), dim=1\n",
    "        ).unsqueeze(-1)\n",
    "        pooled_output = torch.sum(transformer_output * attention_weights, dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "# UPDATED: Enhanced loss function - Focal Loss for better class balance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        frames, labels = batch\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # UPDATED: Use new autocast format to avoid deprecation warning\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # UPDATED: Use gradient scaler for mixed precision training\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate predictions\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return epoch_loss / len(dataloader), accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            frames, labels = batch\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return val_loss / len(dataloader), accuracy, precision, recall, f1\n",
    "\n",
    "def plot_metrics(train_values, val_values, metric_name):\n",
    "    \"\"\"\n",
    "    Plot the training and validation metrics\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_values, label=f'Train {metric_name}')\n",
    "    plt.plot(val_values, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{metric_name.lower().replace(\" \", \"_\")}_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def train_model():\n",
    "    # Create data loaders\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(\"Preparing data...\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_json_path='train_set.json',\n",
    "        val_json_path='val_set.json',\n",
    "        test_json_path='test_set.json',\n",
    "        videos_dir=VIDEOS_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    print(\"Data loaded!\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Create model\n",
    "    print(\"Initializing model...\")\n",
    "    model = TransformerClassifier().to(DEVICE)\n",
    "\n",
    "    # Print model summary\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "    # UPDATED: Use Focal Loss instead of BCE\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    \n",
    "    # UPDATED: Enhanced optimizer with better parameters\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=LEARNING_RATE, \n",
    "        weight_decay=1e-3,  # Increased from 1e-4\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    # UPDATED: Use Cosine Annealing scheduler instead of ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=NUM_EPOCHS,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # UPDATED: Add gradient scaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_model_path = 'best_effect_detection_model.pth'\n",
    "    \n",
    "    # UPDATED: Add early stopping\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # UPDATED: Train with gradient scaler\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, DEVICE, scaler\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "\n",
    "        # UPDATED: Use cosine annealing scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # UPDATED: Print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "        # UPDATED: Enhanced model saving with early stopping\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation F1: {val_f1:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # UPDATED: Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "    # Plot metrics\n",
    "    print(\"Plotting metrics...\")\n",
    "    plot_metrics(train_losses, val_losses, 'Loss')\n",
    "    plot_metrics(train_accuracies, val_accuracies, 'Accuracy')\n",
    "    plot_metrics(train_f1s, val_f1s, 'F1 Score')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    # UPDATED: Use weights_only=True to avoid warning\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = validate(\n",
    "        model, test_loader, criterion, DEVICE\n",
    "    )\n",
    "\n",
    "    print(f\"Test Results - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluation on single sequences\n",
    "def evaluate_sequence(model, video_path, start_frame, num_frames, transform):\n",
    "    \"\"\"\n",
    "    Evaluate a single video sequence for effect detection\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame + i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply transform\n",
    "        frame = cv2.resize(frame, FRAME_SIZE)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if transform:\n",
    "            frame = transform(frame)\n",
    "        else:\n",
    "            # Convert to tensor manually\n",
    "            frame = torch.FloatTensor(frame / 255.0).permute(2, 0, 1)\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Stack frames\n",
    "    if len(frames) < num_frames:\n",
    "        # Pad with zeros if not enough frames\n",
    "        for _ in range(num_frames - len(frames)):\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames_tensor.to(DEVICE))\n",
    "        probability = torch.sigmoid(outputs).item()\n",
    "        prediction = 1 if probability > 0.5 else 0\n",
    "\n",
    "    return prediction, probability\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model = train_model()\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), 'final_effect_detection_model.pth')\n",
    "    print(\"Final model saved!\")\n",
    "\n",
    "# Check total GPU memory\n",
    "print(f\"GPU Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check current memory usage\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check free memory\n",
    "free_memory, total_memory = torch.cuda.mem_get_info(0)\n",
    "print(f\"GPU Memory Free: {free_memory / 1024**3:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
